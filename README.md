
# Procesamiento de Lenguaje Natural.

Este repositorio contiene el desarrollo de los **Desaf√≠os** realizados en el marco de la materia **Procesamiento de Lenguaje Natural (PLN)**, correspondiente a la **Especializaci√≥n en Inteligencia Artificial** de la **Facultad de Ingenier√≠a de la Universidad de Buenos Aires (FIUBA)**.

üë§ **Autor:** Marcelo Adri√°n M√°s Valdecantos  
üìò **C√≥digo de alumno:** a1811  

---

## Contenido del repositorio

### 1Ô∏è Desaf√≠o 1 ‚Äì Vectorizaci√≥n y Clasificaci√≥n de Textos

> **Tema:** Dataset *20 Newsgroups*, vectorizaci√≥n con **TF-IDF**, clasificaci√≥n con **Na√Øve Bayes** y an√°lisis de similaridad entre documentos.

> - Vectorizamos con **TF-IDF** todo el corpus porque ayuda a destacar palabras que son importantes en un documento pero no comunes en todos, luego elegimos al azar 5 documentos.

> - Implementamos el concepto de **"cosine similarity"**, que mide qu√© tan similares son dos vectores en t√©rminos de su √°ngulo para encontrar los 5 documentos m√°s similares.

> - Exploramos:
>   - Distintos modelos de **Na√Øve Bayes**:
>     - `MultinomialNB` (bueno para textos en general)
>     - `ComplementNB` (mejor en conjuntos desbalanceados)
>   - Distintas configuraciones del vectorizador `TfidfVectorizer`:
>     - Variaciones en `ngram_range`, `min_df` y `stop_words`.
>   - Para encontrar la mejor combinaci√≥n que capte mejor el contenido del texto.

> - Finalmente, se explora el impacto de distintas representaciones vectoriales sobre el rendimiento del modelo.
> - Por ultimo:
>     - Buscamos analizar similaridad entre palabras usando la representaci√≥n TF-IDF transpuesta, para trabajar con vectores          de palabras.
>     - Se eligen 5 palabras espec√≠ficas y comprensibles, en lugar de aleatorias, para evitar t√©rminos irrelevantes o muy             poco frecuentes.
>     - Calculamos la similaridad del coseno entre palabras para identificar aquellas que comparten un contexto de uso                similar.
> 
> **-->** Este enfoque demostr√≥ que es posible descubrir relaciones sem√°nticas relevantes entre palabras, basadas en su coaparici√≥n en contextos similares. Adem√°s, al seleccionar palabras manualmente y comprensibles, se facilit√≥ una interpretaci√≥n significativa de los resultados.

--- 

### Desaf√≠o 2 ‚Äì Entrenamiento de Embeddings con Word2Vec

> **Tema:** Crear vectores de palabras utilizando **Gensim** y el modelo **Word2Vec**, entrenado sobre un corpus propio (el poema *Mart√≠n Fierro*). Se analizan relaciones sem√°nticas entre t√©rminos y se visualiza su organizaci√≥n en el espacio de embeddings.


#### Preparaci√≥n del corpus

> - Se ley√≥ el texto *martin_fierro.txt* l√≠nea por l√≠nea.
> - Cada l√≠nea fue tokenizada con `text_to_word_sequence`, convirtiendo el corpus en una lista de listas de palabras limpias (min√∫sculas, sin puntuaci√≥n).
> - El resultado fue un conjunto de **2449 l√≠neas** y un vocabulario de **557 palabras distintas** (tras aplicar `min_count=3`).

#### Entrenamiento del modelo

> - El modelo fue entrenado durante **20 √©pocas**.
> - Se observ√≥ una disminuci√≥n progresiva de la p√©rdida, se√±al de que el modelo aprend√≠a relaciones contextuales entre palabras en el poema.

#### Ensayo del modelo

> - Se utilizaron funciones como `most_similar` y `get_vector` para:
>   - Explorar t√©rminos cercanos en el espacio sem√°ntico.
>   - Visualizar c√≥mo una palabra se representa como vector.
> - Ejemplo: se analizaron palabras como `"el"` y `"cantar"` para verificar la coherencia de los vecinos sem√°nticos aprendidos.

#### Visualizaci√≥n de embeddings

> - Se aplic√≥ **TSNE** para reducir la dimensionalidad a 2D.
> - Se graficaron los **200 vectores m√°s frecuentes** con `plotly`, observando agrupaciones tem√°ticas y sem√°nticas.

> **-->** Este desaf√≠o demostr√≥ c√≥mo entrenar embeddings desde cero sobre un corpus literario permite capturar **relaciones contextuales profundas entre palabras**. A diferencia de m√©todos basados en frecuencia como TF-IDF, Word2Vec genera representaciones que reflejan **la estructura sem√°ntica del lenguaje**, lo que habilita tareas de an√°lisis l√©xico, descubrimiento de sin√≥nimos y visualizaci√≥n de agrupamientos conceptuales.

--- 


### Desaf√≠o 3 ‚Äì Modelo de Lenguaje a Nivel de Caracteres con RNN

> **Tema:** Construcci√≥n y entrenamiento de un modelo de lenguaje basado en caracteres utilizando redes recurrentes. Se entrena el modelo sobre un corpus literario y se eval√∫a su capacidad para generar texto, considerando estrategias de generaci√≥n y el uso de la perplejidad como m√©trica de validaci√≥n.

#### Preparaci√≥n del corpus

> - Se utiliz√≥ nuevamente el texto del *Mart√≠n Fierro*, cargado y preprocesado como una √∫nica secuencia continua.
> - Se defini√≥ un vocabulario de caracteres (`char2idx`, `idx2char`) para tokenizar el texto.
> - El corpus se convirti√≥ en una secuencia de enteros representando cada car√°cter.
> - Se dividi√≥ el conjunto en entrenamiento y validaci√≥n, definiendo un tama√±o de contexto para generar secuencias.

#### Arquitectura del modelo

> - Se construy√≥ un modelo **SimpleRNN** con la siguiente arquitectura:
>   - Capa `TimeDistributed` con codificaci√≥n one-hot por car√°cter.
>   - Capa `SimpleRNN` con 200 unidades, dropout y recurrent dropout.
>   - Capa `Dense` final con activaci√≥n `softmax` para predecir el pr√≥ximo car√°cter.
> - El modelo se compil√≥ con p√©rdida `sparse_categorical_crossentropy` y optimizador `rmsprop`.

---

#### Callback para Perplejidad

> - Se implement√≥ un **callback personalizado** para calcular la **perplejidad** sobre los datos de validaci√≥n al final de cada √©poca.
> - La perplejidad mide la "incertidumbre" del modelo en la predicci√≥n del siguiente car√°cter.
> - Tambi√©n se us√≥ este callback para aplicar **early stopping** cuando la perplejidad dejaba de mejorar.

#### Entrenamiento

> - El modelo fue entrenado con `batch_size=256` durante hasta 20 √©pocas.
> - Se guard√≥ autom√°ticamente el modelo con mejor score de perplejidad.

#### Inferencia interactiva

> - Se carg√≥ el mejor modelo entrenado y se construy√≥ una interfaz con `Gradio` para generar texto.
> - Dada una secuencia de entrada, el modelo predice el siguiente car√°cter usando **greedy search**.
> - Esta herramienta permite probar la capacidad del modelo para continuar textos de forma coherente.

>  **-->** Este desaf√≠o mostr√≥ c√≥mo entrenar un modelo de lenguaje a nivel de caracteres utilizando **redes neuronales recurrentes**, evaluando su desempe√±o con perplejidad y utilizando callbacks personalizados. La tokenizaci√≥n por caracteres permite trabajar con vocabularios reducidos y aprender patrones ortogr√°ficos y gramaticales, aunque requiere m√°s pasos para capturar significado a nivel sem√°ntico. El uso de una interfaz interactiva facilita la exploraci√≥n cualitativa de la calidad del modelo.

---

### Desaf√≠o 4 ‚Äì Bot de Preguntas y Respuestas con Atenci√≥n

> **Tema:** Construcci√≥n de un sistema de generaci√≥n de respuestas (Bot QA) basado en una arquitectura encoder-decoder con atenci√≥n, entrenado sobre el dataset SQuAD. Se integran embeddings preentrenados y t√©cnicas de inferencia como muestreo con temperatura.

#### Implementaci√≥n de la Capa de Atenci√≥n

> - Se defini√≥ una clase personalizada `AttentionLayer` que calcula un **vector de contexto ponderado** a partir de la salida del encoder y la del decoder.
> - Esta capa permite que el decoder **enfoque diferentes partes del input** durante la generaci√≥n de cada palabra de la respuesta.

#### Preparaci√≥n del dataset

> - Se utiliz√≥ el dataset **SQuAD v1.1**, del cual se extrajeron 10.000 pares (pregunta, respuesta) con longitud limitada para facilitar el entrenamiento inicial.
> - Se a√±adieron tokens especiales `<sos>` y `<eos>` a las respuestas.
> - Se utiliz√≥ `Tokenizer` y `pad_sequences` para convertir los textos a secuencias y preparar los datos de entrada y salida para el modelo.

#### Uso de embeddings preentrenados

> - Se cargaron embeddings **FastText** (300 dimensiones) para enriquecer la representaci√≥n sem√°ntica de palabras.
> - Se construy√≥ una `embedding_matrix` alineada con el vocabulario del dataset.
> - Se probaron embeddings fijos y entrenables en distintas versiones del modelo.

#### Arquitectura del modelo

> - Encoder: `Embedding` ‚Üí `Bidirectional LSTM`
> - Decoder: `Embedding` ‚Üí `LSTM`
> - Atenci√≥n: se aplic√≥ sobre las salidas del encoder y decoder.
> - Salida: capa `Dense` con `softmax` aplicada a cada paso temporal.
> - Se entren√≥ el modelo con `sparse_categorical_crossentropy` y `adam`.

#### Entrenamiento

> - El modelo fue entrenado con `batch_size=64` por hasta 200 √©pocas, con validaci√≥n y visualizaci√≥n de precisi√≥n.
> - Se guardaron pesos del modelo y se reusaron para inferencia si ya exist√≠an.

#### Inferencia y generaci√≥n de respuestas

> - Se redefinieron modelos separados para **inferencia del encoder y decoder**.
> - Se implement√≥ una funci√≥n `generate_answer` que genera respuestas mediante:
>   - Tokenizaci√≥n y padding de la pregunta.
>   - Paso por el encoder.
>   - Generaci√≥n palabra a palabra con el decoder.
>   - **Muestreo estoc√°stico controlado por temperatura**, que permite explorar variabilidad en las respuestas.

> Ejemplo de generaci√≥n:
> ```python
> print(generate_answer("Do you read?", temperature=1.2))
> print(generate_answer("Where are you from?", temperature=1.0))
> print(generate_answer("Do you have any pet?", temperature=0.8))
> ```

> --> Este desaf√≠o integr√≥ varios componentes clave de los modelos modernos de NLP:
> - La arquitectura **encoder-decoder con atenci√≥n**, que mejora la calidad de las secuencias generadas al enfocarse en partes relevantes del input.
> - El uso de **embeddings preentrenados** que aportan conocimiento sem√°ntico previo.
> - La capacidad de generar texto de manera controlada mediante estrategias como **muestreo con temperatura**, que permiten balancear coherencia y creatividad.
> 
> En conjunto, se construy√≥ una base s√≥lida para modelos de di√°logo y generaci√≥n de texto m√°s avanzados.

---

## üß∞ Tecnolog√≠as y Herramientas Utilizadas

A lo largo de los cuatro desaf√≠os se emplearon m√∫ltiples tecnolog√≠as del ecosistema de procesamiento de lenguaje natural (NLP), aprendizaje profundo y visualizaci√≥n de datos. A continuaci√≥n se listan las principales:

### Bibliotecas y Frameworks

- **Python 3** ‚Äì Lenguaje principal de desarrollo
- **NumPy** ‚Äì Manipulaci√≥n de arrays y operaciones num√©ricas
- **Pandas** ‚Äì Manejo y estructuraci√≥n de datos tabulares
- **Matplotlib / Seaborn / Plotly** ‚Äì Visualizaci√≥n de resultados y embeddings
- **Scikit-learn** ‚Äì M√©tricas, vectorizaci√≥n (TF-IDF), validaci√≥n cruzada
- **Gensim** ‚Äì Entrenamiento de modelos Word2Vec para embeddings de palabras
- **TensorFlow / Keras** ‚Äì Implementaci√≥n de modelos de redes neuronales:
  - Modelos secuenciales y funcionales
  - Capas recurrentes: `SimpleRNN`, `LSTM`, `GRU`
  - Capas personalizadas (e.g., `AttentionLayer`)
- **Gradio** ‚Äì Construcci√≥n de interfaces interactivas para modelos NLP

### Procesamiento de Texto

- **TF-IDF (`TfidfVectorizer`)** ‚Äì Vectorizaci√≥n cl√°sica de texto
- **Word2Vec (Skip-gram)** ‚Äì Embeddings entrenados desde cero sobre corpus literario
- **Embeddings preentrenados (FastText)** ‚Äì Representaciones vectoriales sem√°nticas externas
- **Tokenizaci√≥n a nivel de palabra y car√°cter** ‚Äì Preparaci√≥n de datos para modelos de lenguaje

### Modelos de Lenguaje

- **Modelos estad√≠sticos supervisados**:
  - `MultinomialNB`, `ComplementNB` (Na√Øve Bayes)
- **Modelos secuenciales**:
  - Modelos de lenguaje a nivel de caracteres
  - Modelos encoder-decoder con atenci√≥n para generaci√≥n de texto

###  Evaluaci√≥n y M√©tricas

- **F1-score macro** ‚Äì Evaluaci√≥n balanceada para clasificaci√≥n multiclase
- **Perplejidad** ‚Äì M√©trica de desempe√±o para modelos de lenguaje
- **Similaridad del coseno** ‚Äì Para comparar documentos o palabras vectorizadas

###  Datasets

- **20 Newsgroups** ‚Äì Clasificaci√≥n de documentos por tema
- **Mart√≠n Fierro (texto literario)** ‚Äì Entrenamiento de embeddings y modelo de lenguaje
- **SQuAD v1.1** ‚Äì Preguntas y respuestas para entrenamiento del bot QA

---

## Sobre la materia

La materia **Procesamiento de Lenguaje Natural (PLN)** aborda t√©cnicas, modelos y algoritmos para permitir que las m√°quinas comprendan, interpreten y generen lenguaje humano. Se estudian desde m√©todos estad√≠sticos cl√°sicos hasta modelos de aprendizaje profundo basados en redes neuronales.

---

## C√≥mo utilizar este repositorio

1. Clonar el repositorio o descargarlo como archivo ZIP.
2. Abrir los archivos `.ipynb` con Jupyter Notebook o Google Colab.
3. Ejecutar las celdas secuencialmente para reproducir los resultados.
4. Revisar los comentarios y outputs en cada notebook para seguir el flujo de an√°lisis.

> Es recomendable crear un entorno virtual e instalar las dependencias necesarias con `pip install -r requirements.txt` si aplica.

---

## Resumen de Resultados y Aprendizajes

- Se abordaron tareas clave de NLP mediante clasificaci√≥n de texto, an√°lisis de similaridad, generaci√≥n de lenguaje y sistemas de QA con atenci√≥n.
- Se experiment√≥ con modelos estad√≠sticos (Na√Øve Bayes), embeddings propios (Word2Vec) y redes neuronales recurrentes con arquitectura encoder-decoder.
- Se fortalecieron habilidades en preprocesamiento, dise√±o de modelos, evaluaci√≥n con m√©tricas espec√≠ficas y visualizaci√≥n de representaciones sem√°nticas.


---

##  Diagrama conceptual (proceso t√≠pico en PLN)

```mermaid
graph TD;
    A[Corpus de Texto] --> B[Preprocesamiento];
    B --> C[Vectorizaci√≥n];
    C --> D[Entrenamiento de Modelo];
    D --> E[Evaluaci√≥n];
    E --> F[Predicci√≥n / Respuesta];
```

Este diagrama representa el flujo com√∫n de trabajo aplicado en los diferentes desaf√≠os del curso.
