
# Procesamiento de Lenguaje Natural.

Este repositorio contiene el desarrollo de los **Desaf√≠os** realizados en el marco de la materia **Procesamiento de Lenguaje Natural (PLN)**, correspondiente a la **Especializaci√≥n en Inteligencia Artificial** de la **Facultad de Ingenier√≠a de la Universidad de Buenos Aires (FIUBA)**.

üë§ **Autor:** Marcelo Adri√°n M√°s Valdecantos  
üìò **C√≥digo de alumno:** a1811  

---

## Contenido del repositorio

### 1Ô∏è Desaf√≠o 1 ‚Äì Vectorizaci√≥n y Clasificaci√≥n de Textos

> **Tema:** Dataset *20 Newsgroups*, vectorizaci√≥n con **TF-IDF**, clasificaci√≥n con **Na√Øve Bayes** y an√°lisis de similaridad entre documentos.

> - Vectorizamos con **TF-IDF** todo el corpus porque ayuda a destacar palabras que son importantes en un documento pero no comunes en todos, luego elegimos al azar 5 documentos.

> - Implementamos el concepto de **"cosine similarity"**, que mide qu√© tan similares son dos vectores en t√©rminos de su √°ngulo para encontrar los 5 documentos m√°s similares.

> - Exploramos:
>   - Distintos modelos de **Na√Øve Bayes**:
>     - `MultinomialNB` (bueno para textos en general)
>     - `ComplementNB` (mejor en conjuntos desbalanceados)
>   - Distintas configuraciones del vectorizador `TfidfVectorizer`:
>     - Variaciones en `ngram_range`, `min_df` y `stop_words`.
>   - Para encontrar la mejor combinaci√≥n que capte mejor el contenido del texto.

> - Finalmente, se explora el impacto de distintas representaciones vectoriales sobre el rendimiento del modelo.
> - Por ultimo:
>     - Buscamos analizar similaridad entre palabras usando la representaci√≥n TF-IDF transpuesta, para trabajar con vectores          de palabras.
>     - Se eligen 5 palabras espec√≠ficas y comprensibles, en lugar de aleatorias, para evitar t√©rminos irrelevantes o muy             poco frecuentes.
>     - Calculamos la similaridad del coseno entre palabras para identificar aquellas que comparten un contexto de uso                similar.
> - Este enfoque demostr√≥ que es posible descubrir relaciones sem√°nticas relevantes entre palabras, basadas en su coaparici√≥n en contextos similares. Adem√°s, al seleccionar palabras manualmente y comprensibles, se facilit√≥ una interpretaci√≥n significativa de los resultados.

--- 

### Desaf√≠o 2 ‚Äì Entrenamiento de Embeddings con Word2Vec

> **Tema:** Crear vectores de palabras utilizando **Gensim** y el modelo **Word2Vec**, entrenado sobre un corpus propio (el poema *Mart√≠n Fierro*). Se analizan relaciones sem√°nticas entre t√©rminos y se visualiza su organizaci√≥n en el espacio de embeddings.


#### Preparaci√≥n del corpus

> - Se ley√≥ el texto *martin_fierro.txt* l√≠nea por l√≠nea.
> - Cada l√≠nea fue tokenizada con `text_to_word_sequence`, convirtiendo el corpus en una lista de listas de palabras limpias (min√∫sculas, sin puntuaci√≥n).
> - El resultado fue un conjunto de **2449 l√≠neas** y un vocabulario de **557 palabras distintas** (tras aplicar `min_count=3`).

#### Entrenamiento del modelo

> - El modelo fue entrenado durante **20 √©pocas**.
> - Se observ√≥ una disminuci√≥n progresiva de la p√©rdida, se√±al de que el modelo aprend√≠a relaciones contextuales entre palabras en el poema.

#### Ensayo del modelo

> - Se utilizaron funciones como `most_similar` y `get_vector` para:
>   - Explorar t√©rminos cercanos en el espacio sem√°ntico.
>   - Visualizar c√≥mo una palabra se representa como vector.
> - Ejemplo: se analizaron palabras como `"el"` y `"cantar"` para verificar la coherencia de los vecinos sem√°nticos aprendidos.

#### Visualizaci√≥n de embeddings

> - Se aplic√≥ **TSNE** para reducir la dimensionalidad a 2D.
> - Se graficaron los **200 vectores m√°s frecuentes** con `plotly`, observando agrupaciones tem√°ticas y sem√°nticas.

> Este desaf√≠o demostr√≥ c√≥mo entrenar embeddings desde cero sobre un corpus literario permite capturar **relaciones contextuales profundas entre palabras**. A diferencia de m√©todos basados en frecuencia como TF-IDF, Word2Vec genera representaciones que reflejan **la estructura sem√°ntica del lenguaje**, lo que habilita tareas de an√°lisis l√©xico, descubrimiento de sin√≥nimos y visualizaci√≥n de agrupamientos conceptuales.

--- 


### 3Ô∏è Desaf√≠o 3 ‚Äì Clasificaci√≥n de Texto Multiclase
> **Tema:** Construcci√≥n de modelos para clasificaci√≥n multiclase.  
>  Incluye selecci√≥n de corpus, ingenier√≠a de caracter√≠sticas, entrenamiento, evaluaci√≥n y optimizaci√≥n de modelos.

### 4Ô∏è Desaf√≠o 4 ‚Äì Sistema de Pregunta-Respuesta
> **Tema:** Desarrollo de un sistema QA (*Question Answering*) basado en modelos de lenguaje preentrenados.  
> Se trabaja con t√©cnicas avanzadas de representaci√≥n contextualizada para responder preguntas sobre un corpus espec√≠fico.

---

## Tecnolog√≠as utilizadas

- Python 3.x
- Scikit-learn
- Pandas / NumPy
- NLTK / spaCy
- Transformers (Hugging Face)
- Jupyter Notebooks

---

## Sobre la materia

La materia **Procesamiento de Lenguaje Natural (PLN)** aborda t√©cnicas, modelos y algoritmos para permitir que las m√°quinas comprendan, interpreten y generen lenguaje humano. Se estudian desde m√©todos estad√≠sticos cl√°sicos hasta modelos de aprendizaje profundo basados en redes neuronales.

---

##  Recursos complementarios

- [Documentaci√≥n de Scikit-learn](https://scikit-learn.org/stable/)
- [Gu√≠a de NLP de Hugging Face](https://huggingface.co/course/chapter1)
- [Curso de PLN de Stanford (CS224n)](http://web.stanford.edu/class/cs224n/)

---

---

## C√≥mo utilizar este repositorio

1. Clonar el repositorio o descargarlo como archivo ZIP.
2. Abrir los archivos `.ipynb` con Jupyter Notebook o Google Colab.
3. Ejecutar las celdas secuencialmente para reproducir los resultados.
4. Revisar los comentarios y outputs en cada notebook para seguir el flujo de an√°lisis.

> Es recomendable crear un entorno virtual e instalar las dependencias necesarias con `pip install -r requirements.txt` si aplica.

---

## Resumen de Resultados y Aprendizajes

- Se aplicaron diversas t√©cnicas de preprocesamiento de texto, vectorizaci√≥n y clasificaci√≥n, lo que permiti√≥ comparar enfoques cl√°sicos y modernos en PLN.
- Se experiment√≥ con **modelos supervisados**, **similaridad sem√°ntica** y **modelos de lenguaje preentrenados**.
- Se adquirieron habilidades pr√°cticas para construir pipelines de procesamiento de texto, incluyendo tareas de clasificaci√≥n, an√°lisis de sentimientos y QA.

---

##  Diagrama conceptual (proceso t√≠pico en PLN)

```mermaid
graph TD;
    A[Corpus de Texto] --> B[Preprocesamiento];
    B --> C[Vectorizaci√≥n];
    C --> D[Entrenamiento de Modelo];
    D --> E[Evaluaci√≥n];
    E --> F[Predicci√≥n / Respuesta];
```

Este diagrama representa el flujo com√∫n de trabajo aplicado en los diferentes desaf√≠os del curso.
